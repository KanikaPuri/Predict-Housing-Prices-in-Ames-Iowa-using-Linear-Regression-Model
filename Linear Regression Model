import pandas as pd

#Read text into dataframe using delimiter 
data = pd.read_csv("AmesHousing.txt",delimiter='\t')
train = data[0:1460]
test = data[1460:]
train.info()
test.info()
target = 'SalePrice'

#Generate 3 Scatter Plots in the same column
import matplotlib.pyplot as plt
# For prettier plots.
import seaborn

plt.scatter(data['Garage Area'],data['SalePrice'],c='green')
plt.scatter(data['Gr Liv Area'], data['SalePrice'],c='red')
plt.scatter(data['Overall Cond'],data['SalePrice'],c='blue')

From the last screen, we can tell that the Gr Liv Area feature correlates the most with the SalePrice column. We can confirm this by calculating the correlation between pairs of these columns using the pandas.DataFrame.corr() method:

train[['Garage Area', 'Gr Liv Area', 'Overall Cond', 'SalePrice']].corr()
              Garage Area  Gr Liv Area  Overall Cond  SalePrice
Garage Area     1.000000    0.468997     -0.151521    0.623431
Gr Liv Area     0.468997    1.000000     -0.079686    0.708624
Overall Cond   -0.151521   -0.079686      1.000000    -0.077856
SalePrice       0.623431    0.708624      -0.077856    1.000000


# Use Scikit-learn to find optimal parameter values for model

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train[['Gr Liv Area']], train['SalePrice'])
print(lr.coef_)
print(lr.intercept_)
a0 = lr.intercept_
a1 = lr.coef_

#Create full working model computing Residual Mean Square Values and inputting them into test_rmse and train_rmse using Linear Regression Model
import numpy as np

lr = LinearRegression()
lr.fit(train[['Gr Liv Area']], train['SalePrice'])
from sklearn.metrics import mean_squared_error

train_predictions = lr.predict(train[['Gr Liv Area']])
test_predictions = lr.predict(test[['Gr Liv Area']])

train_mse = mean_squared_error(train_predictions,train['SalePrice'])
test_mse = mean_squared_error(test_predictions,test['SalePrice'])

train_rmse = train_mse**0.5
test_rmse = test_mse**0.5

#Multivariate Model
cols = ['Overall Cond', 'Gr Liv Area']

lr.fit(train[cols],train['SalePrice'])

train_predictions = lr.predict(train[cols])
test_predictions = lr.predict(test[cols])

mse_1 = mean_squared_error(train_predictions,train['SalePrice'])
mse_2  =  mean_squared_error(test_predictions,test['SalePrice'])
train_rmse_2 = mse_1**0.5
test_rmse_2 = mse_2**0.5

# Feature Selection inside Dataframe data. Drop columns'PID','Year Built','Year Remod/Add','Garage Yr Blt','Mo Sold','Yr Sold'. 

import pandas as pd
data = pd.read_csv('AmesHousing.txt', delimiter="\t")
train = data[0:1460]
test = data[1460:]

numerical_train = train.select_dtypes(include=['int','float'])
numerical_train = numerical_train.drop(['PID','Year Built','Year Remod/Add','Garage Yr Blt','Mo Sold','Yr Sold'], axis =1)
null_series = numerical_train.isnull().sum()
full_cols_series = null_series[null_series==0]


#Compute Pairwise Correlation Coefficients between columns inside training subset defined here. 

train_subset = train[full_cols_series.index]
pairwise = train_subset.corr()
sorted_corrs=abs(pairwise['SalePrice']).sort_values()

#To detect collinearity in candidate features, create a heat map. 

import seaborn as sns
import matplotlib.pyplot as plt
strong_corrs = sorted_corrs[sorted_corrs > 0.3]
corrmat = train_subset[strong_corrs.index].corr()
sns.heatmap(corrmat)

#Clean Data and drop rows with missing values for target

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np
final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])
features = final_corr_cols.drop(['SalePrice']).index
target = 'SalePrice'
clean_test = test[final_corr_cols.index].dropna()
lr=LinearRegression()
lr.fit(train[features],train[target])
train_predictions=lr.predict(train[features])
test_predictions=lr.predict(clean_test[features])
train_rmse = mean_squared_error(train_predictions,train[target])**0.5
test_rmse = mean_squared_error(test_predictions,clean_test[target])**0.5

#Rescale so all values are normalized

unit_train = (train[features] - train[features].min())/(train[features].max() - train[features].min())
print(unit_train.min())
print(unit_train.max())
# Confirmed: the min and max values are 0.0 and 1.0 respectively

sorted_vars = unit_train.var().sort_values()
print(sorted_vars)
